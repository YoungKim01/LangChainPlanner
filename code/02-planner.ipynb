{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "search_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "search_key = os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need \"chain of thought\" style agnet: https://blog.langchain.dev/planning-agents/\n",
    "\n",
    "**Graph State**: In LangGraph, every node updates a shared graph state. The state is the input to any node whenever it is invoked.\n",
    "\n",
    "Below, we will define a state dict to contain the task, plan, steps, and other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding/vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    api_key=azure_openai_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=\"2023-09-01-preview\",\n",
    "    chunk_size=1 \n",
    ")\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_key,\n",
    "    index_name=\"boardai03\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[llm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4\",\n",
    "    api_key=azure_openai_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=\"2023-09-01-preview\",\n",
    "    # temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_key=\"hybrid\", search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "| prompt\n",
    "| llm\n",
    "| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: how do i create a board paper? \n",
      " answer:  To create a board paper, you first need to establish an agenda based on approved topics such as performance, operation, and profit, and email your board members (NEC) about it. The board members will then write the paper. Once you receive their responses and compile them into a paper, you can make necessary amendments based on feedback before circulating the finalized version to all board members.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"User: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    print(\"query:\",query , \"\\nanswer: \", rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb?ref=blog.langchain.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_planner = \"\"\"For the following task, make plans that can solve the problem step by step. For each plan, indicate \\\n",
    "which external tool together with tool input to retrieve evidence. You can store the evidence into a \\\n",
    "variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "\n",
    "(1) RAG+LLM[input]: A pretrained RAG+LLM like yourself. Useful when you need to act with general\n",
    "world knowledge and common sense + specific knowledge and data especially for Board and secretary role. Prioritize it when you are confident in solving the problem\n",
    "yourself. Input can be any instruction.\n",
    "\n",
    "(2) sendEmail_chase_gatherPaper[input]: Worker that send emails to board members to ask write board paper, check status of the return of those email and chase. \n",
    "Input will be a list of board members and agendas.\n",
    "\n",
    "(3) compilePaper[input]:Worker that add all the separate paper into one board paper. input will be multiple board paper pieces from board members.\n",
    "\n",
    "(4) circulatePaper[input]: Worker that send compiled board paper to board members and get feed back and fix. Input will be a board paper. \n",
    "\n",
    "(5) sendReminderEmail[input]: xyz\n",
    "\n",
    "For example,\n",
    "Task: execute this: To create a board paper, first, based on an approved agenda, the secretary sends an email to the NEC (or board members). The NEC then writes the paper. \n",
    "After the paper is written, the secretary compile, circulates and distributes the paper to the board members.\n",
    "\n",
    "\n",
    "Plan: Find out what is approved agenda. #E1 = RAG+LLM[what is approved agenda]\n",
    "\n",
    "Plan: Find out who are the NEC or board members. #E2 = RAG+LLM[who are the NEC or board members]\n",
    "\n",
    "Plan: the secretary sends an email to the NEC (or board members).The NEC writes the paper. #E3 = sendEmail_chase_gatherPaper[#E2]\n",
    "\n",
    "Plan: compilePaper #E4 = compilePaper[#E3] \n",
    "\n",
    "Plan: After the paper is written, the secretary circulates and distributes the paper to the board members. #E5 = circulatePaper[#E4]\n",
    "\n",
    "\n",
    "Begin! \n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\\nQuestion: {question}\n",
    "\\nContext: {context}\n",
    "\\nAnswer:\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_prompt = ChatPromptTemplate(input_variables=['context', 'question'], \n",
    "                            metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, \n",
    "                            messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt_planner))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "| p_prompt\n",
    "| llm\n",
    "| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plan: Determine what the agendas are for the board paper. #E1 = RAG+LLM[what are the agendas for the board paper]\\n\\nPlan: Identify the NEC or board members who will be contributing to the board paper. #E2 = RAG+LLM[who are the NEC or board members]\\n\\nPlan: Send an email to the NEC or board members asking them to write their sections of the board paper based on the determined agendas. Chase up any members who have not responded and compile the responses. #E3 = sendEmail_chase_gatherPaper[#E2, #E1]\\n\\nPlan: Compile all the received board paper pieces into one comprehensive board paper. #E4 = compilePaper[#E3] \\n\\nPlan: Show the compiled board paper to the person in charge and ask if they want to make any fixes. If they say yes, make the specified fixes. #E5 = RAG+LLM[Do you want to fix anything in the compiled board paper?]\\n\\nPlan: If no fixes are needed or once all fixes are made, circulate the final version of the board paper to all board members. #E6 = circulatePaper[#E5] \\n\\nPlan: Confirm that the board paper has been circulated. #E7 = RAG+LLM[Has the board paper been circulated to all board members?]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rag_chain.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan: Determine what the agendas are for the board paper. #E1 = RAG+LLM[what are the agendas for the board paper]\n",
      "\n",
      "Plan: Identify the NEC or board members who will be contributing to the board paper. #E2 = RAG+LLM[who are the NEC or board members]\n",
      "\n",
      "Plan: Send an email to the NEC or board members asking them to write their sections of the board paper based on the determined agendas. Chase up any members who have not responded and compile the responses. #E3 = sendEmail_chase_gatherPaper[#E2, #E1]\n",
      "\n",
      "Plan: Compile all the received board paper pieces into one comprehensive board paper. #E4 = compilePaper[#E3] \n",
      "\n",
      "Plan: Show the compiled board paper to the person in charge and ask if they want to make any fixes. If they say yes, make the specified fixes. #E5 = RAG+LLM[Do you want to fix anything in the compiled board paper?]\n",
      "\n",
      "Plan: If no fixes are needed or once all fixes are made, circulate the final version of the board paper to all board members. #E6 = circulatePaper[#E5] \n",
      "\n",
      "Plan: Confirm that the board paper has been circulated. #E7 = RAG+LLM[Has the board paper been circulated to all board members?]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
